# Fine-tuned Phi-2 Experiment with Needle-in-Haystack Evaluation

experiment:
  name: "yarn_Phi2_needle_haystack_F"
  description: "Fine-tuned Phi2 with YaRN context extension evaluated on needle-in-haystack"
  seed: 42


model:
  type: "Phi2FinetunedModel"              # ‚Üê Key: Use fine-tuned model
  base_path: "microsoft/phi-2"            # Base model
  adapter_path: "checkpoints/phi2_yarn_8k/final_model"  # Your LoRA adapters


# strategy:
#   type: "LinearPIStrategy"
#   original_length: 2048
#   target_length: 8192
#   rope_scaling:
#     type: "dynamic"
#     factor: 8
#   window_size: 8192
strategy:
  type: null


evaluation:
  needle_haystack:
    context_lengths: [2048, 4096, 8192]
    depths: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
    needle: "The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day."
    question: "\n\nBased on the content above, what is the best thing to do in San Francisco?\nAnswer:"
    
    haystack:
      length: 100
      language: "english"
    
    num_trials: 1
    metric: "accuracy"


inference:
  max_tokens: 100
  temperature: 0.1
  top_p: 0.9
  do_sample: false


logging:
  log_dir: "logs"
  log_file: "yarn_TinyLlama_needle_haystack_F.txt"
  level: "INFO"
