# Fine-tuned Phi-2 Experiment with Needle-in-Haystack Evaluation

experiment:
  name: "tinyllama_8k_finetuned_needle_haystack"
  description: "Fine-tuned TinyLlama with LoRA adapters evaluated on needle-in-haystack"
  seed: 42
  date: "2025-11-06"


model:
  type: "TinyLlamaFinetunedModel"              # ‚Üê Key: Use fine-tuned model
  base_path: "TinyLlama/TinyLlama_v1.1"            # Base model
  adapter_path: "checkpoints/tinyLlama_pi_g5/final_model"  # Your LoRA adapters


strategy:
  type: "LinearPIStrategy"
  original_length: 2048
  target_length: 8192
  rope_scaling:
    type: "dynamic"
    factor: 8
  window_size: 8192


evaluation:
  needle_haystack:
    context_lengths: [2048, 4096, 8192]
    depths: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
    needle: "The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day."
    question: "\n\nBased on the content above, what is the best thing to do in San Francisco?\nAnswer:"
    
    haystack:
      length: 100
      language: "english"
    
    num_trials: 1
    metric: "accuracy"


inference:
  max_tokens: 100
  temperature: 0.1
  top_p: 0.9
  do_sample: false


logging:
  log_dir: "logs"
  log_file: "tinyllama_finetuned_experiment.txt"
  level: "INFO"
